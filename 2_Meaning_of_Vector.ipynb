{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2장에서 다루는 내용\n",
    "\n",
    "- 2장에서는 자연어의 의미를 임베딩에 어떻게 녹여낼 수 있는지를 주로 살핀다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2장 벡터가 어떻게 의미를 가지게 되는가\n",
    "    2.1 자연어 계산과 이해\n",
    "    2.2 어떤 단어가 많이 쓰였는가\n",
    "        2.2.1 백오브워즈 가정\n",
    "        2.2.2 TF-IDF\n",
    "        2.2.3 Deep Averaging Network\n",
    "    2.3 단어가 어떤 순서로 쓰였는가\n",
    "        2.3.1 통계 기반 언어 모델\n",
    "        2.3.2 뉴럴 네트워크 기반 언어 모델\n",
    "    2.4 어떤 단어가 같이 쓰였는가\n",
    "        2.4.1 분포 가정\n",
    "        2.4.2 분포와 의미 (1): 형태소\n",
    "        2.4.3 분포의 의미 (2): 품사\n",
    "        2.4.4 점별 상호 정보량\n",
    "        2.4.5 Word2Vec\n",
    "    2.5 이 장의 요약\n",
    "    2.6 참고문헌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 자연어 계산과 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴퓨터는 자연어를 사람처럼 이해할 수 없는 계산기일 뿐이지만, 임베딩을 활용하면 자연어를 계산하는 것이 가능해진다.\n",
    "\n",
    "임베딩: 컴퓨터가 처리할 수 있는 숫자들의 나열인 벡터\n",
    "\n",
    "어떻게? 자연어의 통계적 패턴 정보를 통째로 임베딩에 넣는다.\n",
    "\n",
    "임베딩 구축 시 사용하는 통계 정보 3가지\n",
    "    1. 문장에 어떤 단어가 (많이) 쓰였는지          [빈도]\n",
    "    2. 단어가 어떤 (순서)로 등장하는지            [순서]\n",
    "    3. 문장에 어떤 단어가 (같이) 나타났는지        [동시출현]\n",
    "    \n",
    "|구분     | 백오브워즈 가정    | 언어 모델     | 분포 가정     |\n",
    "|--------|-------------------|----------------|-------|\n",
    "|내용     | 어떤 단어가 (많이) 쓰였는가 | 단어가 어떤 순서로 쓰였는가 | 어떤 단어가 같이 쓰였는가|\n",
    "|대표 통계량| TF-IDF | - | PMI |\n",
    "|대표 모델 | Deep Averaging Network | ELMo, GPT | Word2Vec |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words (BOW) = 어떤 단어가 (많이) 쓰였는지!\n",
    "\n",
    "- 저자의 의도는 단어 사용 여부나 그 빈도에서 드러난다.\n",
    "- 순서 정보는 무시.\n",
    "- 가장 많이 사용하는 것은 TF-IDF / Deep Learning Version = Deep Averagin Network\n",
    "\n",
    "## Language Model = 단어 시퀀스가 얼마나 자연스러운지에 대한 확률\n",
    "\n",
    "- ELMo, GPT 등과 같은 뉴럴 네트워크 기반의 언어모델\n",
    "\n",
    "## Distributional Hypothesis =  문장에서 어떤 단어가 같이 쓰였는지를 고려\n",
    "\n",
    "- 단어의 의미는 그 주변 문맥을 통해 유추 가능\n",
    "- 대표통계량은 점별 상호 정보량 (PMI, Pointwise Mutual Information)\n",
    "- 대표모델은 Word2Vec\n",
    "\n",
    "백오브 워즈 가정, 언어 모델, 분포 가정은 말뭉치의 통계적 패턴을 서로 다른 각도에서 분석하는 것이며, 상호 보완적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 어떤 단어가 많이 쓰였는가\n",
    "\n",
    "### 2.2.1 Bag Of Words\n",
    "\n",
    "> Bag이란 중복 원소를 허용한 집합(Multiset), 순서는 고려하지 않아!\n",
    "\n",
    "{a,a,b,c,c,c} = {c,a,b,c,a,c}, {c,a,c,b,a,c}\n",
    "\n",
    "<b>백오브워즈</b>는 문장을 단어들로 나두고 이들을 중복 집합에 넣어 임베딩으로 활용하는 것이라고 보면 된다.\n",
    "\n",
    "##### 가정: 저자가 생각한 주제가 문서에서의 단어 사용에 녹아있다!\n",
    "\n",
    "### 2.2.2 TF-IDF\n",
    "\n",
    "단순하게 단어 빈도만을 임베딩으로 활용하는 것에는 단점이 있다. \n",
    "\n",
    "'을/를'과 같은 조사와 같은 경우는 어떠한 문서에도 등장하지만, 문서에 대하여 어떠한 내용도 담고 있지 않다.   \n",
    "이를 위해 <b>Term Frequency-Inverse Document Frequency</b>가 필요하다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 단어가 어떤 순서로 쓰였는가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 통계 기반 언어 모델\n",
    "> 언어 모델이란 단어 시퀀스에 확률을 부여하는 모델\n",
    "\n",
    "BOW는 순서를 무시하지만 언어모델은 시퀀스 정보를 명시적으로 학습.\n",
    "단어가 n개라면 P(w1, w2, ...,wn)을 반환한다.\n",
    "\n",
    "잘 학습된 언어 모델이라면 어떤 문장이 그럴듯한지(확률값이 높은지), 주어진 단어 시퀀스 다음 단어는 무엇이 올지 파악.\n",
    "\n",
    "ex)\n",
    "> 누명을 쓰다  (높은 확률)  \n",
    "  누명을 당하다 (낮은 확률)\n",
    "  \n",
    "* n-gram: n개 단어를 뜻하는 용어  \n",
    "    bi-gram: (난폭, 운전) / (눈, 뜨다)  \n",
    "    tri-gram: (누명,을,쓰다) / (초코, 칩, 쿠키)  \n",
    "    ...\n",
    "    \n",
    "n-gram은 n-gram에 기반한 언어 모델을 의미하기도 하여, 단어들을 n개씩 묶어서 빈도를 학습했다는 뜻이기도 하다.\n",
    "\n",
    "* n-gram 허점: \n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 어떤 단어가 같이 쓰였는가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 점별 상호 정보량(PMI, Pointwise Mutual Information)\n",
    "\n",
    "점별 상호 정보량은 두 확률변수 사이의 상관성을 계량화하는 단위다. (완전 독립 시, 0)  \n",
    "독립이라 함은 한 단어의 등장이 다른 단어의 등장에 전혀 영향을 주지 않는 다는 뜻이다.  \n",
    "만일 영향을 준다면 PMI값이 커지게 된다. \n",
    "\n",
    "<img src=\"image/PMI.png\" width=300 height=300 />\n",
    "\n",
    "PMI는 분포 가정에 따른 단어 가중치 할당 기법이기에, PMI 행렬의 행벡터 자체를 해당 단어의 임베딩으로 사용 가능하다.\n",
    "\n",
    "<img src=\"image/word_context_matrix.png\" width=300 height=300 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
